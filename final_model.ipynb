{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bae58c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import re \n",
    "import gradio as gr\n",
    "from transformers import AutoTokenizer, ViTFeatureExtractor, VisionEncoderDecoderModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1d20d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cpu'\n",
    "encoder_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "decoder_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "model_checkpoint = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
    "model = VisionEncoderDecoderModel.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be0deb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(image,max_length=64, num_beams=4):\n",
    "    image = image.convert('RGB')\n",
    "    image = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    clean_text = lambda x: x.replace('<|endoftext|>','').split('\\n')[0]\n",
    "    caption_ids = model.generate(image, max_length = max_length)[0]\n",
    "    caption_text = clean_text(tokenizer.decode(caption_ids))\n",
    "    return caption_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "defca32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "img = Image.open('Image3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af719b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a person laying on the beach with a surfboard \n"
     ]
    }
   ],
   "source": [
    "descr = predict(img)\n",
    "print(descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "682c6a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#person', '#laying', '#beach', '#surfboard']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def convert_to_hashtags(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "\n",
    "    # Convert words to hashtags\n",
    "    hashtags = ['#' + token for token in filtered_tokens]\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "# Example usage\n",
    "hashtags = convert_to_hashtags(descr)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59a24939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.6032012e-02  9.1123432e-03  3.4280807e-02 ... -6.1837919e-02\n",
      "   8.9081414e-02 -2.0151392e-02]\n",
      " [-4.4979587e-02  1.9186940e-02  4.7269151e-02 ... -1.0629752e-02\n",
      "   1.4186645e-02  9.5968824e-03]\n",
      " [-9.9930698e-03  2.8680803e-03  4.5658320e-02 ... -1.2822804e-02\n",
      "  -3.2542644e-03  1.9380139e-02]\n",
      " [-5.3844891e-02  5.6241129e-02  1.2844045e-01 ... -5.4877698e-02\n",
      "  -4.8924148e-02 -3.4847163e-02]\n",
      " [ 2.2477569e-02  5.9771188e-02  4.5491740e-02 ... -1.5824595e-02\n",
      "   1.3204186e-01 -2.9412491e-02]\n",
      " [-9.3013719e-03  4.1709676e-02  5.6247119e-02 ...  3.0675132e-05\n",
      "   7.1854852e-02 -1.1677127e-01]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('tekraj/avodamed-synonym-generator1')\n",
    "embeddings = model.encode(hashtags)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24e03872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings:\n",
      "tensor([[ 0.1145,  0.0768,  0.0263,  ..., -0.1323, -0.0056,  0.3162],\n",
      "        [ 0.0065,  0.1654, -0.0364,  ...,  0.1892,  0.2014,  0.2443]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "# Sentences we want sentence embeddings for\n",
    "sentences = ['This is an example sentence', 'Each sentence is converted']\n",
    "\n",
    "# Load model from HuggingFace Hub\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
    "model = AutoModel.from_pretrained('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
    "\n",
    "# Tokenize sentences\n",
    "encoded_input = tokenizer(sentences, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "# Compute token embeddings\n",
    "with torch.no_grad():\n",
    "    model_output = model(**encoded_input)\n",
    "\n",
    "# Perform pooling. In this case, max pooling.\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'])\n",
    "\n",
    "print(\"Sentence embeddings:\")\n",
    "print(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a0d0dcf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two people sitting on the snow with a cup of coffee.']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "input_sentence = 'Two people sitting on the snow with a cup of coffee'\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "tokenizer = BartTokenizer.from_pretrained('eugenesiow/bart-paraphrase')\n",
    "batch = tokenizer(input_sentence, return_tensors='pt')\n",
    "generated_ids = model.generate(batch['input_ids'])\n",
    "generated_sentence = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(generated_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4eb6cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkapu\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2304: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A person laying on a beach with a surfboard.\n",
      "['#person', '#laying', '#beach', '#surfboard']\n",
      "A person laying on the beach with a surf board.\n",
      "['#person', '#laying', '#beach', '#surf', '#board']\n",
      "A person lying on the beach with a surf board.\n",
      "['#person', '#lying', '#beach', '#surf', '#board']\n",
      "Person sitting with a surfboard on the beach.\n",
      "['#person', '#sitting', '#surfboard', '#beach']\n",
      "A person laying with a surfboard on the beach.\n",
      "['#person', '#laying', '#surfboard', '#beach']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")  \n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Vamsi/T5_Paraphrase_Paws\")\n",
    "\n",
    "sentence = descr\n",
    "\n",
    "text =  \"paraphrase: \" + sentence + \" </s>\"\n",
    "\n",
    "encoding = tokenizer.encode_plus(text,pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids, attention_masks = encoding[\"input_ids\"].to(\"cpu\"), encoding[\"attention_mask\"].to(\"cpu\")\n",
    "\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids=input_ids, attention_mask=attention_masks,\n",
    "    max_length=256,\n",
    "    do_sample=True,\n",
    "    top_k=120,\n",
    "    top_p=0.95,\n",
    "    early_stopping=True,\n",
    "    num_return_sequences=5\n",
    ")\n",
    "\n",
    "for output in outputs:\n",
    "    line = tokenizer.decode(output, skip_special_tokens=True,clean_up_tokenization_spaces=True)\n",
    "    print(line)\n",
    "    hashtags = convert_to_hashtags(line)\n",
    "    print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b9c4b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#two', '#people', '#snow', '#cup', '#coffee', '#stand']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def convert_to_hashtags(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Remove stopwords and punctuation\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token.lower() for token in tokens if token.isalpha() and token.lower() not in stop_words]\n",
    "\n",
    "    # Convert words to hashtags\n",
    "    hashtags = ['#' + token for token in filtered_tokens]\n",
    "\n",
    "    return hashtags\n",
    "\n",
    "# Example usage\n",
    "hashtags = convert_to_hashtags(line)\n",
    "print(hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b36e6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cactode/gpt2_urbandict_textgen_torch\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"cactode/gpt2_urbandict_textgen_torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3dcd6fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "464dc33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ -1.9520, -10.0520,  -4.4291,  ...,  -5.0669,  -7.1313,  -0.2235],\n",
      "         [ -6.2522, -15.8066,  -5.1867,  ..., -13.4463, -12.4652,  -2.3737],\n",
      "         [ -6.0018, -17.0299,  -9.8854,  ..., -12.0795, -17.0473, -11.6971],\n",
      "         [ -0.7992, -16.7028,  -9.4771,  ..., -14.3147, -11.8018,  -6.5331],\n",
      "         [ -0.3515, -12.4749,  -5.8269,  ...,  -8.5130, -10.8677,  -5.3507],\n",
      "         [  2.4188, -13.0126,  -8.3715,  ..., -16.4383, -14.8155,  -3.0127]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "28793e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "API_URL = \"https://api-inference.huggingface.co/models/cactode/gpt2_urbandict_textgen_torch\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "def query(payload):\n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    return response.json()\n",
    "\n",
    "output = query({\n",
    "    \"inputs\": \"Can you please let us know more details about your \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f34701f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'error': 'Internal Server Error'}\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699a7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pkapu\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:1132: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"salesken/text_generate\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"salesken/text_generate\").to(device)\n",
    "\n",
    "input_query=\"tough challenges make you stronger.  \"\n",
    "input_ids = tokenizer.encode(input_query.lower(), return_tensors='pt').to(device)\n",
    "\n",
    "sample_outputs = model.generate(input_ids,\n",
    "                                do_sample=True,\n",
    "                                num_beams=1, \n",
    "                                max_length=1024,\n",
    "                                temperature=0.99,\n",
    "                                top_k = 10,\n",
    "                                num_return_sequences=1)\n",
    "\n",
    "for i in range(len(sample_outputs)):\n",
    "    print(tokenizer.decode(sample_outputs[i], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b9d965",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
